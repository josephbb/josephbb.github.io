<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joebakcoleman.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joebakcoleman.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-20T21:24:51+00:00</updated><id>https://joebakcoleman.com/feed.xml</id><title type="html">blank</title><subtitle>Of Fish and Fascists </subtitle><entry><title type="html">I did not want to write another blog post.</title><link href="https://joebakcoleman.com/blog/2024/nosek/" rel="alternate" type="text/html" title="I did not want to write another blog post."/><published>2024-11-19T23:36:10+00:00</published><updated>2024-11-19T23:36:10+00:00</updated><id>https://joebakcoleman.com/blog/2024/nosek</id><content type="html" xml:base="https://joebakcoleman.com/blog/2024/nosek/"><![CDATA[<h1 id="quick-backstory-and-why-were-back-here">Quick backstory and why we’re back here</h1> <p>If you’re new to this story I’d recommend checking out Gelman’s <a href="https://statmodeling.stat.columbia.edu/2024/09/26/whats-the-story-behind-that-paper-by-the-center-for-open-science-team-that-just-got-retracted/#comments">write-up</a> summarizing the history behind the <a href="https://www.nature.com/articles/s41562-024-01997-3">retraction</a>. As per the journal, the paper was retracted due to:</p> <blockquote> <p>lack of transparency and misstatement of the hypotheses and predictions the reported meta-study was designed to test; lack of preregistration for measures and analyses supporting the titular claim (against statements asserting preregistration in the published article); selection of outcome measures and analyses with knowledge of the data; and incomplete reporting of data and analyses.</p> </blockquote> <p>In lay terms: outcome switching, covering it up, data-dredging/cherry-picking, and selective reporting.</p> <p>As I noted in a <a href="https://joebakcoleman.com/blog/2024/protzko/">previous blog-post</a>, I’d hoped that the retraction would do enough to set the record straight. Since then, several of the authors have done the rounds in various news outlets framing this as a mistake attributable to accidentally incorporating a single line in the main text. In doing so, they’ve not engaged with the more substantive concerns that warranted retraction, nor the causal issues we raised in the matters arising. They’ve done this in <a href="https://www.nature.com/articles/d41586-024-03178-8">Nature</a>, the <a href="https://www.wsj.com/science/nature-human-behavior-study-retracted-standards-b589dbe6">Wall Street Journal</a> and most recently on <a href="https://podcast.clearerthinking.org/episode/235/brian-nosek-highs-and-lows-on-the-road-out-of-the-replication-crisis/">Spencer Greenberg’s podcast</a></p> <h3 id="the-need-to-set-things-straight">The need to set things straight</h3> <p>It would be great to drop this and not write about it ever again. But here’s the rub: We were asked by the editors to summarize the reason for retraction in our matters arising, and in misrepresenting why the paper was retracted the authors are publicly claiming my published work is incorrect. Here it is on display in <em>Nature</em></p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sciencehardpng.png" sizes="95vw"/> <img src="/assets/img/sciencehardpng.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Here, Nosek attributes the findings of the investigation to <em>us</em>. The investigation involved four domain experts, several editors, and a whole bunch of higher-ups in Springer Nature (as is policy). The result is that a very well known scientist is publicly asserting my work isn’t robust. Critically, they have offered precisely zero evidence to support their claims. Here, we’ll go through the relevant evidence that is publicly available. Instead, as best as I can tell, they are just lying. Given this has now led to misleading articles in multiple higher-profile venues, it’s probably time to set the record straight.</p> <h1 id="fact-checking-the-greenberg-podcast">Fact-checking the Greenberg podcast</h1> <p>There is a secondary and quite interesting story about why, precisely, the journalists who have interviewed the authors have simply taken their word for it and rarely pushed back. It’s not exactly the norm for high-profile retractions, but it has given the authors a bit of a bully pulpit to work from. Spencer to his credit, pushes back a little, but didn’t seem to know the facts well enough to really get to the root of thigns. As his podcast was the longest and least-cropped interview, It’s helpful for sorting fact from fiction.</p> <h3 id="one-pesky-inaccurate-sentence">One pesky inaccurate sentence?</h3> <p>On the reason for retraction, the authors keep emphasizing that a single sentence was inaccurate and they agree the paper needed to be retracted as a result. This is just untrue. Per COPE guidelines retractions occur for issues substantial enough that the findings are no longer reliable. Here’s Nosek echoing this when speaking to greenberg.</p> <blockquote> <p>The paper was challenged as published in 2023. Some attentive readers were surprised to read a statement in it that said every analysis reported here in the individual experiments… Every one of those experiments was pre-registered, and the meta project, these analyses aggregating all of the results across these 80 experiments, was pre-registered. That was a definitive statement that is false. It was in our paper claiming that we pre-registered all these things, so these reforms that we’ve been talking about and promoting. They raised that in a commentary, saying, “In addition to substantive critiques of, we don’t think you can conclude that you observed high replicability for X, Y, and Z reasons. We think the design wasn’t appropriate for testing that question for these reasons.” There’s lots of substantive critique in the ordinary variety, but it opened with this observation of a fundamental critique of failure of this pre-registration statement.</p> </blockquote> <h3 id="or-a-host-of-them">Or a host of them.</h3> <p>What Brian is obscuring by emphasizing this sentence is that the paper affirmatively described what was pre-registered in three separate parts of the paper. It’s easy to see how a sentence accidentally found its way in, much harder to see how these authors would make that mistake thrice. Descriptions of what analyses were confirmatory appear in the main text and supplement, in sections called “confirmatory analysis”. Per Brian’s <a href="https://www.pnas.org/doi/10.1073/pnas.1708274114">previous work</a> “confirmatory analysis” implies the presence of a preregistration (e.g. prediction vs. postdiction).</p> <p>The main text listing of confirmatory analyses contains three subheadings. The first of the analyses described is a meta-analysis added to the paper in spring of 2020 by James Pustejovsky while revising the paper with Protzko. As the data were analyzed and originally <a href="https://www.metascience2019.org">presented</a> in fall of 2019 (we’ll get back to this), this cannot have been preregistered:.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pustejovsky.png" sizes="95vw"/> <img src="/assets/img/pustejovsky.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The supplement’s <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-023-01749-9/MediaObjects/41562_2023_1749_MOESM1_ESM.pdf">confirmatory analysis section</a> (P.38-39) similarly affirms that replicability rate was pre-registered, and notes other post-hoc or deviated analyses as confirmatory studies and even includes a previously-explicitly-exploratory analysis as confirmatory (Lab-specific Variation, <a href="https://osf.io/8p6ra">page 7</a>). It even includes a section called “pre-registered analyses that <em>quite sparsely</em> describes what they actually <a href="https://osf.io/938rv/registrations">preregistered in 2018 and again in 2019</a>.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preregdecline.png" sizes="95vw"/> <img src="/assets/img/preregdecline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>As icing on the cake, they even claimed their expert prediction study was pre-registered. I think that is a grand claim, given this was the <a href="https://osf.io/3yhbe/registrations">preregistration</a>:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/expertpredict.png" sizes="95vw"/> <img src="/assets/img/expertpredict.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>In the text (supplement), the results of this study are reported as follows:</p> <blockquote> <p>The Spearman correlation between prediction accuracy and observed effect size was .104, p = .712, two-tailed.</p> </blockquote> <p>So here, their statements of preregistration refer to a frozen wiki-page describing a wholly different DV (averaged accuracy) tested with a non-parameteric test against an unmentioned IV (effect size). Does this seem preregistered? Did they forget here too, to check what they had planned?</p> <p>Returning to Brian’s version of events, the reasons “statements” was plural in the retraction note is that the paper is replete with multiple affirmations of preregistered analyses that were not, in fact, preregistered or not preregistered as described.</p> <p>The deviations raise a broader question… how could they not notice that any of the analyses involved deviations and disclose them? In 2023 when writing up the paper. Even if that line snuck in, you’d think someone would say</p> <blockquote> <p>“Hey hoss, didn’t we change this analysis to remove the decline effect bit in revision, should we disclose that?”</p> </blockquote> <p>In focusing on a single sentence, it is much easier to sell the story that this was a big accident.</p> <h2 id="but-the-preregistration-is-beside-the-point">But the preregistration is beside the point.</h2> <p>Yet the preregistration claims alone aren’t the reason the paper was retracted. As the retraction notice states, the pregistration claims matter in relation to a broader pattern of failing to be transparent about the meta-project’s original purpose. The authors have repeatedly denied that this finding of the investigation is correct. In the podcast, Spencer asks Brian whether the authors engaged in outcome switching and failed to disclose the original intent of the study. Here’s Spencer’s question and Brian’s response.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/originalpurpose.png" sizes="95vw"/> <img src="/assets/img/originalpurpose.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Brian here is arguing that decreasing the prominence of the decline-effect related analysis and motivation is not an indication that the study switched its motivations. He says its reasonable for us to think that, but goes on to say it isn’t the case.</p> <p>Fortunately, I didn’t rely on vibes when raising these concerns. Instead, I just looked at the documents they shared. Tal Yarkoni, in his review at <em>Nature</em> sensed the outcome switch, noting:</p> <blockquote> <p>Third, I found the recurring “decline” theme throughout the article fairly puzzling. It seems poorly motivated and at odds with the overall framing of the manuscript as a test of whether or not replicability is possible in psychology…Again, if the intent here is to test for some kind of supernatural observer effect, then that should be clearly disclosed, and the reader can decide for themselves how to interpret that.</p> </blockquote> <p>This makes it clear that in the version reviewed and rejected at <em>Nature</em> the authors had transparency issues surrounding clear design choices intended to test supernatural effects. Yarkoni sensed it, likely knowing Schooler’s past work, and asked the authors to be honest about it.</p> <p>And it’s not just Tal Yarkoni. Reviewer Daniel Lakens also sensed that the project was about the decline effect but not being clear about that fact.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lakensdecline.png" sizes="95vw"/> <img src="/assets/img/lakensdecline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Here’s the author’s response to Yarkoni:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reviewerresponsedecline.png" sizes="95vw"/> <img src="/assets/img/reviewerresponsedecline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Brian’s statement makes it as though the outcome switching was gleened from reading the tea-leaves of relative emphasis of replicability vs. the spooky decline effect. Yet, here, the authors explicitly state that they lessened the prominence of these hypotheses because the findings were null. Switching from a null outcome to one that supports a wholly different claim is… well… outcome switching.</p> <p>Schooler’s hypothesis, which is the reason the project was funded, is only described as “According to one theory, declines in ESs over time are caused by a study being repeatedly run.” This doesn’t make it clear it is one of the author’s theories, and that it motivated the funding, design, preregistration(s) and analysis code. Similarly, the SOM only refers to these original hypotheses as “unusual explanations.” You have to dig deeper in the supplemental materials before you start scratching your head wondering what “observer effects” are.</p> <p>To top it off, the authors elected not to release the code corresponding to these preregistered analyses with the original paper (another oversight?). They also removed effects from the published code which corresponded to the decline effect as preregistered. Between the response to reviewers and the current description of the pre-registered hypotheses and their motivation, severe outcome switching and failure to disclose original intent is unambiguous.</p> <p>To really drive this home, here’s a hastily made collage of the revision edits showcasing a purging of “the decline effect” from the text. They even purge “the passage of time” from the main text, consistent with their undisclosed removal of terms from statistical models which indicate interest in the passage of time. This seems like a pretty active attempt, in response to Yarkoni’s concerns, to distance the project from the very things they preregistered.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/declinecollage.png" sizes="95vw"/> <img src="/assets/img/declinecollage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Now, here’s the kicker. As the authors note, they didn’t want to “dwell” on the decline effect. Yet the supplement contains text lifted from the preregistrations, so somehow they had to forget what was in the preregistrations while… reading and revising text containing aspects of the preregistrations (which are, incidentally, not clearly identified as such).</p> <h3 id="of-course-we-planned-to-study-replicability">“Of course we planned to study replicability”</h3> <p>The reality is that there is very little doubt what they set out to measure and, by extension, what was <em>not</em> planned <em>a priori</em>. The reason we know this is that the authors actually did a fantastic, thoughtful and thorough job preregistering the supernatural tests. From the earliest documentation on the project through two-pre-registrations the hypotheses remained the same. They hired a statistics consultant to turn these hypotheses into tests. They even wrote <a href="https://osf.io/9gew8">code, blind to data</a>, specifying precisely what analyses they planned. Code they modified substantially post hoc for the main analysis, and then just did not share.</p> <p>If you download that code, the 09-03-2019 version has an option <code class="language-plaintext highlighter-rouge">use_real_data</code> set to <code class="language-plaintext highlighter-rouge">TRUE</code>. This makes it clear that they indeed analyzed the data, using this code, on September 3rd of 2019. As of their initial analysis of the data, on code they spent months preparing, following a preregistration that closely matched the original grant proposal from years earlier… they did not measure replicability. They don’t even calculate whether the results are significant for any of the experiments or replications! There is no indication of anything other than the supernatural and observer-based hypothesis tests until the very moment they analyzed the preliminary data.</p> <p>In the podcast, Brian takes a fairly tortured line of reasoning that they didn’t think it was necessary to preregister the meta-project. The original replication efforts from ~2015 weren’t, so why would they think to preregister these? Well, that too is a bit of an obfuscation because other replication efforts projects since then have been preregistered. Most memorably, Many Labs 4 which… <a href="https://osf.io/preprints/psyarxiv/ejubn">resulted in concerns the authors deviated from their preregistration in ways that made the finding spicier.</a>. Elsewhere, the early (2013) start date of the project is implied as a reason for not preregistering but the pre-registrations for this project were completed in 2018 and 2019.</p> <p>Are we to believe that this was the plan all along and the lead authors and stats consultant simply forgot over two years of developing the analysis plan and code, only to remember after analyzing the data? That they didn’t believe meta-studies needed to be pre-registered even though just months later Nosek was involved in a project that <a href="https://osf.io/xsp6g/registrations?view_only=1b7e38df8d414545b4a229f027620122">thoroughly preregistered</a> which metrics would be used to evaluate replicability? It strains credulity.</p> <p>This argument that they did not think it was necessary raises more questions than it answers, and I’m surprised journalists haven’t pushed back. If that was the case, wouldn’t they have known when revising in 2023 and then simply said that in response to reviewers? If, as Brian suggests, they wrestled with ideas of how difficult it was to determine how to measure replicability—where is that in the manuscript? Why not be transparent and instead present everything as planned all along? Elsewhere, the authors get into similar problems asserting that they didn’t realize how many choices they would have during analysis. Why not describe this? Their version of events requires us to believe they did not think it was necessary to preregister, realized how many analytic choices they had, and inadvertently wrote it up as though they’d preregistered… while simultaneously believing they had planned this all along but simply forgot to preregister.</p> <p>If you have any doubt the replication analyses were posthoc, check out the <a href="https://joebakcoleman.com/blog/2024/protzko/">first part</a> of this series that I hope will end with this post. Also, you can read the retraction note which affirms this is all true following a lengthy long investigation in which the authors had ample time to provide any documentation to the contrary. While I was not privy to any of this, I imagine even an email from early 2018 saying “here’s how were we measuring replicability and demonstrating it is high” would have gone a long way. Indeed in the interview and elsewhere the authors seem to indicate they didn’t realize how complicated it would be to select metrics until they analyzed the data—they were indeed selected with knowledge of the results! If you feel the journal erred in their decision, please write up why!</p> <h2 id="how-did-this-happen">How did this happen?</h2> <p>The last thing that needs clarity is the “no idea how it happened” refrain we keep hearing. Here it is in Greenberg’s podcast:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/howdidithappen.png" sizes="95vw"/> <img src="/assets/img/howdidithappen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>I’ll assume Brian hasn’t looked at the documentation they added, because we know exactly how this happened and why.</p> <p>The version submitted to <em>Nature</em> did not have a link to the decline effects preregistration, and did not indicate everything was preregistered. Yarkoni noted this seemed strange, given the overall focus of the paper.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preregcommentreview.png" sizes="95vw"/> <img src="/assets/img/preregcommentreview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>So how did that pesky line get added? Well, in the revision for resubmission it was added by John Protzko.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/protzkoeditprereg.png" sizes="95vw"/> <img src="/assets/img/protzkoeditprereg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>First, we keep hearing this “many cooks in the kitchen” excuse but the document history reveals that the vast majority of revisions relevant to the retraction were done by John Protzko. Or from his logged-in Microsoft word account.</p> <p>Notably, the historical documents indicate that John Protzko drafted the original preregistration, used a hired consultant for feedback, went through rounds of revision, preregistered it in 2018, then summarized it again in a second preregistration in 2019. The edit history shows he was actively involved in the addition of post-hoc <a href="https://joebakcoleman.com/blog/2024/protzko/">results and changing metrics</a>. At some point in time, the pre-registered code got cribbed over to the main analysis code, modified and several additional analyses were added.</p> <p>Against this backdrop, when asked explicitly by reviewers if <em>these analyses were preregistered</em>, this line was added. When they were asked by two reviewers to clarify if they had intended to study a supernatural phenomena—they removed virtually all references to the original hypothesis and its motivations.</p> <h1 id="hanlons-razor">Hanlon’s Razor</h1> <p><a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor">Hanlon’s razor</a> has been along for quite the ride with this paper. It states:</p> <blockquote> <p>Never attribute to malice that which is adequately explained by stupidity.</p> </blockquote> <p>Nosek’s story of the retraction invites us to conclude honest error (Hanlon’s stupidity). It tells a nice and tidy story of a plan to conduct analyses of these sort since 2013, that somehow went missing from the pre-registration because no one realized you should preregister such things <em>back then</em>. When write-up happened, this little line snuck its way in–what with all the co-authors and stuff. It invites us to marvel at the folly of so many authors in not noticing.</p> <p>Yet Hanlon’s razor begins to strain when you realize the line Nosek emphasizes was written by the very same author who penned and submitted the two pre-registrations across multiple drafts. That the pre-registrations were done in 2018, not 2019. That the very same author had worked with a hired statistics consultant to produce analysis code, blind to data, which made no mention of replicability. It’s hard to see this line as an accident when variants of misrepresented preregistration occur not just in this line but in two additional sections and for an entirely different experiment–also “preregistered” by the very same lead author. It gets somewhat harder still to assume honest error when the line was added in response to reviewers who found themselves surprised no explicit claims of preregistration for the meta-project were made (only “confirmatory analyses”).</p> <p>Perhaps we view all of this and <em>still</em> see incompetence rather than malice. Now we have to grapple with how incompetence could result in the authors systematically and very meticulously removing from the main text any indication they set out to study supernatural effects. This clearly, as per the authors response to reviews, occurred in response to two reviewers sensing it and asking them to come clean. That they forgot, when revising to refer to fringe theories, those theories originated with last author on the paper–even as they cited the work he wrote on the topic. That none of this occurred to them when Lakens and Yarkoni’s reviews seemed to <em>sniff out</em> the outcome switching in 2020. “Nope, no outcome switching here… we preregistered everything… right guys?”</p> <h2 id="returning-to-the-interviews">Returning to the interviews</h2> <p>In the Wall Street Journal <a href="https://www.wsj.com/science/nature-human-behavior-study-retracted-standards-b589dbe6">article</a>, Leif stated “It wasn’t because we were trying to fool someone, but it is because we were incompetent”. The piece in <a href="https://www.nature.com/articles/d41586-024-03178-8">Nature</a> quotes Brian saying, “I don’t know how many times I read that paper with these erroneous claims about everything being preregistered and missed it. It was just a screw up.” The piece, in its title, indicates the authors are doing some soul-searching.</p> <p>If we still believe incompetence, not malice, to be the case, than the incompetence appears to continue to this day. Brian seems to have somehow missed during the investigation and multiple read-throughs that more than one statement was inaccurate. He seems to not be aware that the documents his team released make it unambiguous who added that line. he doesn’t know that the preregistrations occurred in 2018 and 2019, replete with blind analysis code that made no indication of an intent to study replication. He missed or forgot that the authors, during revision, explicitly told the reviewers they were distancing themselves from the original hypothesis.</p> <p>At this point, I’m personally inclined to believe the authors are choosing to continue the very fabrications that motivated the retraction in the first place. It seems they are still lying, and misleading journalists much as they did readers about what they planned to study, what they preregistered, and the history of the project as a whole. That—much like in their response to reviewers—they’re still choosing to ignore the substantive critique and spin things in a positive light. I can see why the incentives would push things this way—its very hard to admit you engaged in the very same thing you’ve built careers rallying against.</p> <p>The authors are welcome to prove me wrong. They can start with the unambiguous stuff. They can reach out to journalists and correct their statement that a single line motivated the retraction, acknowledging the other two sections as well. They can clarify that it was the journal’s finding, not ours and ensure that the pieces acknowledge daylight between the motivation for retraction and our critique on the causal claims which do not depend at all on the issues with preregistration. They can clarify for Spencer that they, at least now, know exactly how that pesky line wound up in the paper and sort out what led Protzko to forget the two preregistrations when writing it.</p> <p>Then they can work towards the ahrder bits… acknowleding that it isn’t the relative prominence of in the article that leads to a conclusion of outcome switching, but two preegistrations and analysis code applied to data which make no mention of their ostensibly long-running plan to analyze replicability. They can point the journalists towards evidence which clearly indicates many of these analyses were post hoc, with knowledge of the data, evolved over time. They can clarify that they actively, in response to reviewers, removed language indicating Schooler’s hypothesis that catalyzed the whole project. In short? They can take ownership of the outcome switching, post-hoc analyses presented as being <em>a priori</em>, and hacking and cherry-picking of metrics. If they want to defend their findings, they should start from acknowledging earnestly how they arrived at them.</p> <p>I won’t hold my breath. Early on, I emailed the authors and offered to walk them through this. In the wake of WSJ, I emailed Leif and sent along the detailed concerns that I had originally shared with the journal. I wrote a <a href="http://joebakcoleman.com/blog/2024/protzko/">past blog post</a> that at least one of the authors certainly read, pointing them to unambiguous evidence. My offer to hop on a Zoom and walk them through the issues was first made a year ago, and it still stands. If they don’t trust me… <a href="https://www.chronicle.com/article/this-study-was-hailed-as-a-win-for-science-reform-now-its-being-retracted">Stephanie Lee</a> covered a lot of this as well. Perhaps they could chat with her, as they have before on other cases of science gone awry.</p> <p>So far, there has been no engagement with any of the evidence and only continued statements that conflict with it. Perhaps they just have a tiny bit more soul-searching to do.</p>]]></content><author><name></name></author><category term="metascience"/><category term="statistics"/><category term="retraction preregistration"/><category term="openscience"/><category term="preregistration"/><category term="metascience"/><summary type="html"><![CDATA[The Preregistration Revelation Part II]]></summary></entry><entry><title type="html">I did not want to write this blog post.</title><link href="https://joebakcoleman.com/blog/2024/protzko/" rel="alternate" type="text/html" title="I did not want to write this blog post."/><published>2024-09-24T23:36:10+00:00</published><updated>2024-09-24T23:36:10+00:00</updated><id>https://joebakcoleman.com/blog/2024/protzko</id><content type="html" xml:base="https://joebakcoleman.com/blog/2024/protzko/"><![CDATA[<h1 id="the-preregistration-revelation">The Preregistration Revelation</h1> <p>I did not want to write this blog post. I hate when science gets resolved by blog and thread. This is why I emailed the authors in November of last year. I shared very detailed concerns and offered to walk them through those concerns so they could retract on their own terms. They declined. Instead, I got the runaround familiar to anyone asking for data available upon request. This was frustrating but not yet alarming. What really concerned me was this post by Brian Nosek:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nosekbegins.png" sizes="95vw"/> <img src="/assets/img/nosekbegins.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>To anyone reading, this seems like a quite reasonable narrative. After all, it isn’t a grave sin to forget to preregister a descriptive analysis or two. From my perspective, it was quite different. I had emailed the authors and provided a very clear outlined draft and code comparison indicating that virtually none of the analyses matched the preregistration. One or two analyses is a mistake, the entire main text is a <em>problem</em>. Brian had to have known the issues were much more substantial than he let on, and posted in a way that minimized the seriousness of the issues.</p> <p>I still did not want to write this blog post, but I no longer felt confident the authors would earnestly evaluate the concerns. I wrote a dozen pages carefully detailing the issues and privately shared them with the editors. This was entirely distinct from the Matters Arising. Berna was not involved. I asked that my role in sharing these concerns not be made public, although said it could be shared with the authors to facilitate resolution. I thought perhaps the gravity would convince them to engage with the concerns and voluntarily retract. They did not. They did, however, upload a large trove of historical documentation as part of the investigation I was not privy too.</p> <p>Examining these new files confirmed my concerns and raised many, many more. We wound up <em>way</em> past a simple issue whereby they forgot to preregister some or even all analyses. I felt compelled to send an additional write-up of my findings to the editors. For those unfamiliar with the process, I had heard nothing back about anything the authors or experts reviewing the concerns had said or contributed. Late this summer, I was informed that the investigation had concluded and they found my concerns were well-founded. The paper was retracted not over the issue of preregistration, but over a much deeper set of concerns that raise serious ethical questions and most importantly undermine the claims of the paper.</p> <p>At this point, I thought surely the authors would own up to their mistakes and I would not have to write this blog post. All along, I have hoped the authors would acknowledge their mistakes and move on. I held out some shred of belief that people who have built careers on honesty, transparency, and openness could be honest, open, and transparent. I was naive.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nosek.png" sizes="95vw"/> <img src="/assets/img/nosek.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>They are misrepresenting the reason for retraction. They’re presenting it as a minor procedural issue with a sentence describing what was preregistered, and some deviations they failed to disclose. Worse, people on social media are believing this and applauding them for their honesty and contrition. Yet this is all bullshit and spin. The paper would not have been retracted, nor would I have raised concerns, if this was the only issue. It wouldn’t have taken 10 months and twenty pages of documentation.</p> <p>Mostly, I don’t have the time to write this. I work a 9-to-5, am writing a book, and have science to do. My website broke so I’ve slapped it up temporarily on a placeholder website… I don’t have time to fix it. I suspect the editors, my co-authors, reviewers, and ethics board members who spent countless hours trying to understand what the authors did and correct the record felt much the same. <a href="https://en.wikipedia.org/wiki/Brandolini%27s_law">Brandolini’s law</a> in full force. I am writing it because it is only fair to their time to share this, given the authors unwillingness to portray what happened honestly.</p> <p>Because I’m short on time you’ll find typos. You may find errors. You may need to dig around on their OSF to verify things. Most of the relevant files can be found in this <a href="https://osf.io/rnvxk">zip</a> which tragically I cannot link to individually. If you’re wondering whether you can trust this, I can safely say this is a subset of the things I shared with the editors and I have heard and seen nothing from the authors or team at NHB to suggest anything I raise here is incorrect.</p> <p><em>Without further ado</em></p> <h1 id="calling-bullshit">Calling Bullshit</h1> <p>If you read the <a href="https://www.nature.com/articles/s41562-024-01997-3">retraction note</a>, the editors do not mince words about why this study was retracted:</p> <ol> <li>“lack of transparency and misstatement of the hypotheses and predictions the reported meta-study was designed to test”</li> <li>“lack of preregistration for measures and analyses supporting the titular claim (against statements asserting preregistration in the published article)”</li> <li>“selection of outcome measures and analyses with knowledge of the data”</li> <li>“incomplete reporting of data and analyses.”</li> </ol> <p>How could a sentence describing all analyses as preregistered result in anything beyond the 2nd reason given? Why would a journal make such strong statements over a sentence and a few deviations? No paper has ever been retracted—if even corrected—for issues that small. Let’s go through these one by one, as these reasons are what the authors have long described as questionable research practices (QRPs).</p> <p>The first is <strong>Outcome Switching</strong>. The authors switched the decade-long-planned outcome of the study from an null finding for a supernatural phenomenon to one on replication discovered after analysis that affirmed their long-standing advocacy and was <a href="https://www.cos.io/blog/the-reforms-are-working">marketed to funders.</a></p> <p>The second issue is <strong>Falsification</strong>. They repeatedly—even in the note they just published—made false claims about their research process. We often think of falsifying data, but false claims about study methodology–including preregistration—is falsification.</p> <p>The third is <strong>HARKing</strong>. They analyzed their data, noticed a descriptive result and wrote the paper around it. They conducted statistical tests after having viewed the data that they are still claiming were preregistered (see below).</p> <p>The final issue is <strong>selectively reporting</strong> of analyses and data. They have utterly refused to provide essential insight into their piloting phase, despite the issue having been raised at least four years ago in peer review and again during the investigation. The relevant repositories remain private.</p> <p>If you simply read the retraction notice the journal put out rather than the authors’ statement it is clear <strong>the paper was rejected because the authors were found to have engaged extensively in QRPs</strong>. Of course questionable research practices are <em>questionable</em> but when a group of experts on the subject engage in them repeatedly to push a claim they fundraise on it makes you start to question. It is very important to realize that, much like a business school professor selling books based on fraud… several key authors have every reason to generate and protect this finding.</p> <h1 id="feeling-not-so-hot-honest-open-and-transparent">Feeling Not so HOT (Honest, Open and Transparent)</h1> <p>In the authors’ recently released <a href="https://osf.io/4k5sf">OSF post mortem</a>, Brian seems to suggest that the problems mostly surrounded a single line in the paper. Tellingly their explanation involves no engagement with the substance of the retraction notice or the matters arising. What possible reason, other than obfuscation, would there be not to even address the journal’s decision as they wrote it? Instead, he focuses on something we mention only in passing. Something much less ethically fraught that he has been saying since November of last year. The inaccuracy of this line in their paper:</p> <blockquote> <p>“All confirmatory tests, replications and analyses were preregistered both in the individual studies (Supplementary Information section 3 and Supplementary Table 2) and for this meta-project (https://osf.io/6t9vm).”</p> </blockquote> <p>Read the authors’ post mortem in contrast with the journal’s exact language for why the retraction occurred. It is downright deceptive to describe the retraction as primarily a result of this sentence. The issue they highlight as the cause for retraction is only a minor point that could have been fixed with a correction. No one, myself included, felt this line alone was cause enough to retract from the beginning. It would not have taken ten months and four reviewers to sort this out. I understand that if authors disagree with the retraction—most authors of most retractions do—yet this is no excuse for failing to engage substantively with the journal’s rationale and create the impression the cause was quite different.</p> <p>The authors post-game write-up, along with spinning an offer to submit a fresh manuscript to fresh peer review as an R&amp;R has been very effective spin. Commentators on twitter appear to think this just isn’t a big deal. Everyone is making a mountain out of a molehill and the authors are setting a good example by “retracting, revising and resubmitting”</p> <p>What is striking is that <strong>even now</strong> the authors are choosing to obfuscate what they actually preregistered. Later in <a href="https://osf.io/2s94g/">the note</a>, they explain how this specific language came about and highlight that elsewhere they’ve described things accurately:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/manyconfirmatory.png" sizes="95vw"/> <img src="/assets/img/manyconfirmatory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>“Many” might leave you with the impression that this section accurately described the preregistered analyses. Nope. Let’s take the very first analysis in that confirmatory analysis section. It describes a multi-level meta-analysis that was coded up by James Pustejovsky:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/confirmatoryanalysis.png" sizes="95vw"/> <img src="/assets/img/confirmatoryanalysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>You can even check out the <a href="https://osf.io/938rv/">pre-registered analysis code which describes this analysis as exploratory</a>. As a fun aside, this preregistered code was developed by a <a href="https://osf.io/rnvxk">paid consultant</a> they hired to provide feedback on the pre-registration and develop the pre-registration code. There’s a whole <a href="https://osf.io/rnvxk">version history</a> ostensibly created without access to data leading up to the presentation of results at Metascience 2019. Importantly, none of this code was released with original manuscript and only came to light after I raised the issue when sumbitting my concerns for investigation. So somehow in addition to forgetting what exactly they preregistered when writing it up, they forgot entirely that about this whole side-plot with a consultant they paid (who became a co-author) and just neglected to upload the code which would have made it much more obvious that the analyses were not preregisterd. What bad luck!</p> <p>Returning to the meta-analysis.</p> <p>You know how we really know this meta-analysis is exploratory though? Because <a href="https://osf.io/rnvxk">an old version</a> of the paper clearly indicates they added this analysis in March of 2020 (“Decipfhering the Decline Effect P6_JEP.docx”)—well after Schooler <a href="https://www.youtube.com/watch?v=TUiUA5O0PFk">gave a talk</a> on a preliminary analysis of the data. Conducting an analysis after having analyzed the data and observed pattern is, by the authors own definitions, <a href="https://www.pnas.org/doi/10.1073/pnas.1708274114">not a confirmatory test</a>. It’s a QRP to present it as one. It’s worth highlighting that this meta-analysis is different from their analysis of variation in replicability across labs across labs, which their correction notice indicates was <em>also</em> exploratory and relabeled as confirmatory…</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pustejovsky.png" sizes="95vw"/> <img src="/assets/img/pustejovsky.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Hell, later in their published post-mortem they (maybe?) allude to this analysis not being preregistered by saying that “some sensitivity and heterogeneity analyses” were not preregistered. But that doesn’t fix the earlier language which seems to imply this section described the preregistered analyses. Perhaps what they were thinking of was the section of <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-023-01749-9/MediaObjects/41562_2023_1749_MOESM1_ESM.pdf">their supplement</a> called “pre-registered analyses” which <em>does</em> describe the preregistration accurately. Unfortunately this is within a section also called confirmatory analyses which misleadingly claims that the replication rate and the exploratory-turned-confirmatory lab-specific variation were confirmatory. Why does this section differ from the main text? Who knows? Might be hard to keep your story straight about what you preregistered if you’ve abandoned your prereg long ago.</p> <p>All this is to say that if they mistakenly described what was preregistered, they did it in three distinct places and forgot to upload the code corresponding to the actually preregistered analysis which they describe accurately in the supplement. None of the authors managed to notice this, or the fact that the paper contained multiple inconsistent descriptions of what was preregistered. I don’t care if they had a secret desire to analyze replicablity, in what world do more than a dozen experts–two of whom built platforms to make it easy and careers on encouraging it—forget what they preregistered this severely.</p> <p>Hanlon’s razor is having a hard time with this one. I really don’t know if it’s more alarming if they intentionally mislead the readers or if a team of experts on preregistration can’t even notice if the two sections labeled confirmatory analyses are consistent—and match what they paid a consultant to develop. Unethical behavior here could be limited to this paper or maybe the papers of a few key authors. Yet if this is the extent to which this group of <em>experts</em> can check their own manuscript in revision for three years for <em>internal</em> consistency… even after the paper has been retracted… there is a mountain of literature someone needs to go back and check.</p> <h1 id="it-is-not-just-about-the-preregistration">It is not just about the preregistration</h1> <p>Hopefully the above makes it dreadfully clear that the authors are either unable or unwilling to be honest, open and transparent about what they preregistered, even now. Yet if this had just been a need to clarify which analyses were exploratory, we would have seen a correction in January and this whole thing would be a distant memory. The paper was retracted for many more reasons, a small subset of the easiest to explain are described below.</p> <h2 id="outcome-switching-and-lying-by-omission">Outcome Switching and lying by omission.</h2> <p>One of the key reasons this paper was retracted is that the authors do not accurately describe their original, pre-registered motivation for the study. It is one thing to forget to preregister but it is entirely different to simply switch outcomes and pretend you were studying that thing all along.</p> <p>Stephanie Lee’s story covers the <a href="https://www.chronicle.com/article/this-study-was-hailed-as-a-win-for-science-reform-now-its-being-retracted">supernatural hypothesis</a> that motivated the research and earned the funding from a parapsychology-friendly funder. Author Jonathan Schooler had long ago proposed that merely observing a phenomenon could change its effect size. Perhaps the other authors thought this was stupid, but that’s a fantastic reason to either a) not be part of the project or b) write a separate preregistration for what <em>you</em> predict. We can see how the manuscript evolved to obscure this motivation for the study. The authors <em>were</em> somewhat transparent about their unconventional supernatural explanation in the early drafts of the paper from 2020:</p> <blockquote> <p>“According to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect)25. According to this account, the more studies run between the confirmation study and the self-replication, the greater the decline should be.”</p> </blockquote> <p>This is nearly verbatim from the preregistration:</p> <blockquote> <p>According to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect). Thus, we predict that the more studies run between the confirmation study and the self-replication, the greater will be the decline effect.</p> </blockquote> <p>It is also found in <a href="https://osf.io/rnvxk">responses to reviewers</a> at Nature, who sensed the authors were testing a supernatural idea even though they had reframed things towards replication by this point:</p> <blockquote> <p>“The short answer to the purpose of many of these features was to design the study a priori to address exotic possibilities for the decline effect that are at the fringes of scientific discourse….”</p> </blockquote> <p>As an aside, it’s wild to call your co-authors and funder the fringes of scientific discourse. Why take money from and work with cranks? Have some dignity. Notably, perhaps to keep the funders happy, they were generous enough to keep this idea around in the in the name of their OSF repo and the <a href="https://osf.io/rnvxk">original title</a>:</p> <blockquote> <p>The replicability of newly discovered psychological findings over repeated replications</p> </blockquote> <p>It’s also in their <a href="https://osf.io/rnvxk">original discussion</a>:</p> <blockquote> <p>Importantly, the number of times the initial investigators replicated an effect themselves is not predictive of the replicability of the effect by independent teams (Kunert, 2016).</p> </blockquote> <p>This utterly batshit supernatural framing erodes en route to the the published manuscript. Instead, the authors refer to these primary hypotheses that date back to the origin of the project as phenomena of secondary interest and do not describe the hypotheses and mechanisms explicitly. They refer only to this original motivation in the supplement of “test of unusual possible explanations.”</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/secondaryinterest.png" sizes="95vw"/> <img src="/assets/img/secondaryinterest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Why not? Well, no need to speculate. The authors say exactly why in their response to Tal Yarkoni who asked them in review at Nature to come out and say it if they were testing a supernatural hypothesis. They decided to just change the message of the paper.</p> <blockquote> <p>“As such we revised the manuscript so as not to dwell on the decline effect hypotheses and kept that discussion and motivation for the design elements to the SOM.”</p> </blockquote> <p>It’s fine to realize your idea was bad, but something else to try bury it in the supplement and write up a whole different paper you describe in multiple places as being preregistered and what you set out to study. Peer review is no excuse for misleading readers just to get your study published because the original idea you were funded to study was absurd.</p> <p>Nevertheless, when you read the paper, you’d have no idea this is what they got funding to study. Their omitted variables and undisclosed deviations in their main-text statistical models make it even harder to discern they were after the decline effect. They were only found in the pre-registered analysis code which was made public during the investigation.</p> <p>In distancing themselves for <a href="https://www.fetzer-franklin-fund.org/projects/deciphering-the-decline-effect-1/">two of the three reasons they got funding</a> they mislead the reader about what they set out to study and why. This isn’t a preregistration issue. This is outcome switching, and lying. It’s almost not even by omission because they say it’s the fringes of scientific discourse but it’s the senior author on the paper!</p> <p>The third reason they note in early documents, most closely aligned with their goals is the “creation of a gold standard” for conducting research. While this very much aligns with the message of the paper, it is really important to note that the evaluation criteria boils down to if they fail to observe the supernatural effect they’ll have succeeded. It’ like arguing that if I don’t find a unicorn today, I’ve solved cold fusion. See our matters arising if you want to sort out why this is bad logic.</p> <p>The authors seem to indicate in their post-mortem analysis that they had planned this framing and analysis all along. If so, it’s damn surprising that none of this came up in the investigation. Why not include a link to literally any evidence of the claims in this paragraph? You think if you’re on the hook for outcome switching and HARKing you could toss in a URL to provide evidence to the contrary.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bestlaidplans.png" sizes="95vw"/> <img src="/assets/img/bestlaidplans.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Perhaps they’re referring to their <a href="https://osf.io/ahdy7/files/osfstorage">operations manual</a> but I certainly hope not. There is no plan in that document that matches the description above. If you think the decline thing is absurd and you’re just doing this to study replication… why not mention what you’re going to measure it in any of the dozen versions of the operations manual or the two pre-registrations? Instead, Here’s their early predictions, make of them what you will:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/predictions.png" sizes="95vw"/> <img src="/assets/img/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>If there is some time-stamped documentation that clearly states a desire to measure replicability—much less how they’ll do so—I cannot fathom why they did not produce it during the investigation or when I reached out in November. Perhaps it exists, but other evidence suggests otherwise. What evidence? Well, I’m glad you asked.</p> <h2 id="harking-hacking-and-cherry-picking">HARKing, Hacking and Cherry-Picking</h2> <p>What really chaps my ass about the tweet indicating this is a procedural error is that it somehow ignores their decade of arguing that failing to preregister risks analyses that are conditioned on chance patterns in the data. If you alter your analysis in response to data to achieve a desired result, that result is less trustworthy. Isn’t that the whole story? And sure, exploratory research has value but repeatedly claiming your exploratory data-dependent research is confirmatory is a big, big problem. As <a href="https://www.pnas.org/doi/10.1073/pnas.1708274114">Nosek 2018 put it</a>:</p> <blockquote> <p>“Why does the distinction between prediction and postdiction matter? Failing to appreciate the difference can lead to overconfidence in post hoc explanations (postdictions) and inflate the likelihood of believing that there is evidence for a finding when there is not. Presenting postdictions as predictions can increase the attractiveness and publishability of findings by falsely reducing uncertainty.”</p> </blockquote> <p>By extension, repeatedly claiming that you’re doing prediction when you’re doing postdiction misleads your audience into believing you found more than you did. It’s deceptive and it is wrong.</p> <p>So did they alter their analysis in response to data? Sure seems like it.</p> <p>Replicability was not found in the original preregistration. Nor is it found in the <a href="https://osf.io/rnvxk">analysis code dated 09/03/2019</a>. The first mention of estimating replicability was during Schooler’s talk on 09/05/2019, which estimated replicability as the proportion of significant results among the studies with significant confirmatory tests.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/schoolertalk.png" sizes="95vw"/> <img src="/assets/img/schoolertalk.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>As with past work, they do not emphasize this rate for null confirmatory tests. Instead, they seemed <em>surprised</em> by the rate of significance for null findings. Schooler frames it as an <em>incline effect</em>.</p> <p>Between Schooler’s talk and the first draft of the write-up they wind up with <a href="https://osf.io/rnvxk">this abstract and general set of conclusions</a> from their analysis:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/originalabstract.png" sizes="95vw"/> <img src="/assets/img/originalabstract.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This abstract for the earliest version of the manuscript states the replication rate is 92.2% (59/64) based on a definition that considers null replications of null confirmatory studies as “successful”.</p> <p>There are a few things to note. Their original conclusion is that “high-powered pre-registered studies can be effectively replicated by high-powered pre-registered replications”. This makes sense because they are replicating the confirmatory studies in the original drafts framing—which were high powered. In the published version they’ve broadened this conclude that “This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries.” Where did that framing come from? If this was the goal all along you’d think whoever wrote this draft would be aware.</p> <p>Their definition of replicability used in the final version is not referred to here as replicability but simply statistically significant results. The reason they’re not calling this replicability is because initially they’re considering the outcome of the confirmatory study when estimating replicability. This accounts for the “highly powered pre-registered studies” language in their original abstract.</p> <p>The trouble is that this 92.2% appears to be a mathematical error, as it includes 47 of the 52 replications of significant confirmatory studies and appears to include all 12 replications of null confirmatory studies regardless of outcome. When corrected, this definition would produce a replicability rate would be 79% (51/64). This would be considerably lower than this drafts’ chosen method of calculating statistical power (~100%), and lower than replicability in the published version (86%). It would also be lower than the highest estimates from the literature cited in the initial version and later omitted for reasons unstated (11/13 or 85% from Klein et al 2014). These are the two arguments they based the title on and they fall apart once you fix this little calculation error.</p> <p>But that shiny 84.4% rate of significance is sitting right there! So now, it’s no longer the confirmatory studies they’re replicating but the predicted direction from the pilots they refuse to make public. This gets around the “incline effect” and awkwardness of cases like the Redemption study’s apparent direction reversal. The trouble is that this is not a metric anyone would plan <em>a priori</em> because it is far too sensitive to the base rate. Imagine they had only found 8 “true” findings. 100% significant results in replication for these true findings and 100% replication nulls for the null confirmatory studies. Their measure would indicate 50% replicability—the same as they say is typical in the literature. When you have perfect agreement between statistical decisions arising from replications and confirmations this metric just winds up just measuring the base rate. If it’s 10%? Replicability is 10%. This measure only indicates high replicability because the base rate is so high.</p> <p>Ok, but its not just this choice of measure that is a problem. As versions go on, they do a few things to bring their replicability above their power and the replicability in the literature. First, they revise their estimate of statistical power in a very strange way such that statistical power is zero when the sign of confirmatory test differs from predicted value even though the tests are two-tailed and the theoretical minimum is 5%. They lump in the near-1.0 statistical power estimates for significant confirmatory studies with this weird, at times zero, measure for null studies and goose statistical power down below their absurd and clearly opportunistic replicability estimate. Taken together, their claim that their rate of replication exceeds the theoretical maximum only arises as a result of jointly evolving their estimate of power and replicability. This is hacking statistics to make a claim.</p> <p>Finally, they even alter what they define as the literature-wide rate of replicability they compare against. A key claim is that they observed high replicability and the literature has not. In the main text cite a range of replicability (30-70%) which they seem to average at 50%. You might think the approximately 50% here is just the halfway point between 30 and 70. Nope, the talk by [Schooler] lays out their original math.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fortyseven.png" sizes="95vw"/> <img src="/assets/img/fortyseven.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are plenty of problems with this, not the least of which is that several of these studies involve non-random sampling frames and the estimate of replicability is a result of the chosen mix of well-replicated and novel findings. But the real rub is Many Labs 1. 10 out of 13 is 77%. In reality it’s <a href="https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178">11 out of 13</a> or 85% using their criteria of significance. Ooops, high replicability was achieved a decade ago replicating the open literature. Can’t have that.</p> <p>From the initial analysis to subsequent versions, they cite ML2 and ML3 but all of a sudden leave out this study which would have undermined that claim they were higher than the disappointing range in the literature. Their 92.2% might have looked better, but the 86% is right there with it. They also opt to leave out <a href="https://journals.sagepub.com/doi/abs/10.1177/0956797619831612">Soto 2019</a> which has 87% successful replicability. It’s hard to make a claim that rigor-enhancing techniques are necessary for replicability when two other studies have replicated portions of the literature at similar rates which most assuredly did not embrace these techniques in discovery. Perhaps they didn’t know Soto’s work, but they certainly knew Many Labs 1. The two projects shared authors. If there was some justified reason for selectively excluding mid write-up the one study in the literature that makes it impossible for them to make the claim… that would be a stroke of luck. They seemed to have invented the informal comparison equivalent of selective removing of outliers. Maybe they’d argue it’s not a QRP because it isn’t a formal statistical test, but that didn’t stop them from anchoring the titular claim on this comparison.</p> <p>So they cherry-picked their point of comparison, selectively omitted two conflicting points of comparison from their original framing, and hacked their estimates of power and replicability to be near one another. We really don’t have to speculate that they cherry-picked. They come out and say the quiet part loud in an <a href="https://osf.io/rnvxk">earlier draft</a>. Here, their 75% confidence interval is lower than their their other metrics and they suggest that because it <em>should</em> have been high it should no longer be used as a metric.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lowest.png" sizes="95vw"/> <img src="/assets/img/lowest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="null-hacking">Null hacking??</h2> <p>During the investigation, the authors uploaded an apparently undisclosed sensitivity analysis of some sort (Decline effects/Decline effects - temporal decline and blinding.Rmd). The file appears last modified on March 15th, 2020 shortly after the earliest provided draft of the paper (March 14th, 2020). This code involved four different specifications from the preregistered model, evaluated in various ways: without the blinding effect, without self-replications, independent replications only, and controlling for independent replications.</p> <p>Although these analyses are not described in the published version, they appear to explore the possible model space to (generously) probe whether the results are consistent across model specifications. In the variation that ignores self-replications, the authors do find an interaction effect between wave and blinding that is significant and consistently so across model specifications (see below). It is unclear why the model space was explored so thoroughly and yet these analyses (and their conflicting results) were not included in the write-up. It also seems like there’s gotta be some name for running a model a bunch of different ways like a specification analysis and selectively presenting the null results. Oh wait, that’s Null Hacking <a href="https://osf.io/preprints/psyarxiv/9y3mp">(Protzko 2018)</a></p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nullhacking.png" sizes="95vw"/> <img src="/assets/img/nullhacking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="omission">Omission</h2> <p>Frankly, all of the above could probably be weaseled around. Is it really a formal analyses? These are just descriptives. Everyone engages in QRPs so what’s the big deal here? Maybe the results are still good? Isn’t this obviously true so who cares if the authors embraced all of those things the authors are arguing need to be eradicated for science to work…</p> <p>But here’s the <em>key</em> reason the findings couldn’t be trusted, even if you think everything above is a nothing-burger. It requires the tiniest bit of causal inference understanding, specifically that causes occur before effects.</p> <p>The paper contends that the authors discovered a set of highly replicable findings when using rigor-enhancing best practices. Because the discoveries occurred during the piloting phase, this can <em>only</em> be logically true if they followed the practices in piloting. Practices observed in replication cannot have caused the set of discoveries which were committed to before confirmation and replication.</p> <p>You can read more about the problem in depth as written <a href="http://users.eecs.northwestern.edu/~jhullman/Hullman_Protzko_et_al_comments.pdf">by Hullman</a>, who reviewed the Matters arising and the concerns. Tal Yarkoni, reviewing an earlier submission at Nature, pointed out the need to be transparent about the piloting process 4 years ago. Despite the centrality of the piloting to the effects discovered, the authors have refused to make information about pilots openly and transparently available. They have been silent in response to both Tal and Jessica’s concerns.</p> <p>As you read the paper, ask yourself the following questions:</p> <ul> <li>Did they verify all pilots were pre-registered and conducted without deviations?</li> <li>How many hypotheses were tested per pilot?</li> <li>Were they allowed to enter exploratory, post-hoc analyses based on trends in the pilot?</li> <li>What were the actual sample sizes of the pilots?</li> <li>Etc…</li> </ul> <p>Why is there so little clarity on the process that caused the discoveries? What few pilots I’ve found by weird google searches suggested they were free to test a bunch of outcomes and pick one of the hypotheses to pull forward. It’s clear they didn’t embrace the practice of large sample sizes (1500) in piloting. Worse, it seems they were free to embrace analytic flexibility during pilots as per a post of Nosek’s. He seems to suggest that rigor-enhancing best practices were not strictly required during the pilot–contrary to claims in the paper. We also get an indication that effect sizes declined where we’d expect them to and the authors really didn’t feel the need to mention this in the write-up.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nosexplore.png" sizes="95vw"/> <img src="/assets/img/nosexplore.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Think about this for a second. If they found 16 hypotheses dredging out significant findings from some unstated number of n=200 experiments, committed to replicating them, and got 80% replicability or whatever… it implies that best practices are very much not needed to find replicable results. It seems you can data-dredge and hack significant findings from pilots at n=200, publish them and rest assured the replication crisis is over if the replicators use large sample sizes.</p> <p>This is a <em>Huge</em> question on which the entire paper depends. Tal Pointed it out in his review. James, their hired consultant, brought up the pilot selection problem in comments on drafts of the <a href="https://osf.io/rnvxk">original preregistration in ~2018</a>. Why ignore your hired statistican, reviewers, and the concerns that might retract your paper and <em>still</em> refuse to click “make public” on those pilot repositories? Their post-mortem suggests they’re analyzing the data, they must be organized somewhere, so why are they so reluctant to make them openly available?</p> <p>This was a key reason for retraction, as indicated in <a href="http://users.eecs.northwestern.edu/~jhullman/Hullman_Protzko_et_al_comments.pdf">Hullman’s review</a>. The authors have been aware of the concerns for 4-6 years. It was emphasized in our matters arising in raised in the concerns that prompted the investigation. Against this backdrop, it seems pretty damn misleading for their post-mortem to say:</p> <blockquote> <p>Also, our findings spurred interest in what occurred in the pilot phase of the project. We are likewise quite interested to learn more about the pilot phase.</p> </blockquote> <p>It is very difficult to square this with openness, honesty and transparency.</p> <h1 id="putting-it-all-together">Putting it all together</h1> <p>As I said at the outset. I had no desire to write this blog post. I didn’t want to wade into all of this. Their OSF is such a mess that getting claims straight and conveying them to others is a nightmare. Many of the links above will just open a massive zip file you’ll have to navigate to find the corresponding file. Good luck, go with god. Other parts of their repo including pilots of are unlinked and can only be found by searching on the internet archive, finding a registration and working your way back up—often to a still private repository. And despite the fact that embargos for many pilots should have lifted if they were actually preregistered, I can’t find most by searching in this manner. At times, preregistration seem to be little more than word documented uploaded to an OSF that they could easily remove or edit. Sometimes it’s just a description in a wiki.</p> <p>I have little doubt some will remain unconvinced because this post was tl;dr and the files are td;dr (too disorganized, didn’t read). I expect the authors and probably Lakens to try to seize on a thing or two I got wrong somewhere in here because the repo is a mess and they haven’t addressed any of these concerns. The inevitable bad faith response this will trigger is one of the very many reasons I did not want to write this post. I hate looking forward to some long screed of a blog post where a red square says “Oh but they measured replication in the only logical way to do so, I am not very impressed by Joe’s scientific abilities” or whatnot. Perhaps the authors saying they appreciate my concerns but disagree and will provide documentation. Same thing I’ve been hearing since November.</p> <p>All this is to say if I’m wrong about anything I’m happy to correct or amend. But even if one or two things could be written or framed differently, a lot would have to be wrong for it to meaningfully change the story. Still, if you doubt what’s above, rest assured that this is an abbreviated version of 20+ pages concerns I raised which were independently verified by a ten month investigation involving numerous domain and ethics experts.</p> <p>Whatever your thoughts, ask yourself: How many changes, updates, switched analyses, outcomes, rewrites would it take to call this more than just an “embarrassing mistake”? How many mistakes are we allowed until we cannot shrug them of brazenly? At what point should we be held accountable for allowing mistakes to accumulate over years despite repeated feedback? How many “oopsies” would it take before we interpret their “aw shucks” posts in the last few days as lies intended to cover up their actions?</p> <p>If it’s fewer than I’ve espoused here, I’m happy to share plenty more. This post contains what I consider the bare minimum to convey that they are not being honest. The authors forced my hand, after nearly a year of avoiding making this all public. I could have written this post at any time, if my goal was just to sling mud. Yet their utterly misleading description of why the paper was retracted undermines my attempting to address this through the proper channels. It undermines the hundreds of hours experts spent unravelling this shit when they could have been doing literally anything else. Despite the extra work in comparison to slapping up a blog post, I reported these concerns formally, privately, and quietly because <em>I really did not want to write this post</em>.</p>]]></content><author><name></name></author><category term="metascience"/><category term="statistics"/><category term="retraction preregistration"/><category term="openscience"/><category term="preregistration"/><category term="metascience"/><summary type="html"><![CDATA[The Preregistration Revelation]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://joebakcoleman.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://joebakcoleman.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://joebakcoleman.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>