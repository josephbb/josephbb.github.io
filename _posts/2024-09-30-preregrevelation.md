---
layout: post
title: I did not want to write this blog post. 
date:   2023-10-25
description: The Preregistration Revelation
tags: openscience preregistration metascience
categories: [metascience, statistics, retraction preregistration]
---

# The Preregistration Revelation 

I did not want to write this blog post. I hate when science gets resolved by blog and thread. This is why I emailed the authors in November of last year. I shared very detailed concerns and offered to walk them through those concerns so they could retract on their own terms. The declined, 
and gave me a runaround familiar to anyone asking for data available upon request. They took to social media to spin my concerns as having forgotten to register a few descriptive results. 

Hoping to avoid writing this exact blog post, I took my concerns privately and in detail to the journal alongside our matters arising. I though perhaps the gravity would convince them to actually engage with the concerns and voluntarily retract. Once the paper was retracted and the jouranls 10 month long investigation unambiguously validated the concerns I raised, I thought surely they would own up to their mistakes and I would not have to write this blog post. All along, I have hoped the authors would acknowledge their mistakes and move on. I held out some shred of belief that people who have built careers on honesty, transparency, and openness could be honest, open, and transparent. I was naive. Mostly, I don't have the time to write this. I work a 9-to-5, am writing a book, and have science to do. I suspect the editors, my co-authors,  reviewers, and ethics board members who spent countless hours trying to understand what the authors did and correct the record felt much the same. Because I'm short on time you'll find typos. You may find errors. You may need to dig around on their OSF to verify things. Most of the relevant files can be found in this [zip](https://osf.io/rnvxk) which tragically I cannot link to individually. 

I still do not want to write this post. But here it is... because the authors are unwilling to accurately represent the facts, and that reflects poorly on everyone who was forced to spend countless hours trying to figure out what they had done and how to correct the record.

*Without further ado*

# Calling Bullshit
If you read the [retraction note](https://www.nature.com/articles/s41562-024-01997-3), the editors do not mince words about why this study was retracted:

 1. "lack of transparency and misstatement of the hypotheses and predictions the reported meta-study was designed to test"
 2. "lack of preregistration for measures and analyses supporting the titular claim (against statements asserting preregistration in the published article)"
 3. "selection of outcome measures and analyses with knowledge of the data"
 4. "incomplete reporting of data and analyses."

These are what the authors have long described as questionable research practices. 

The first is **Outcome Switching**. The authors switched the mey outcome of the study from an null finding for a supernatural phenomenon to one on replication that affirmed their long-standing advocacy and helped (https://www.cos.io/blog/the-reforms-are-working)[sell their work to funders.] 

The second issue is **Falsification**. They repeatedly---even in the note they just published---made false claims about their research preregistration.  

The third is **HARKing**. They analyzed their data, noticed a descriptive result and wrote the paper around it. They conducted statistical tests after having viewed the data that they are still claiming were preregistered (see below). 

The final issue is **selectively reporting** of analyses and data. They have utterly refused to provide essential insight into their piloting phase, despite the issue having been raised at least four years ago in peer review and again during the investigation. The relevant repositories remain private. 

If you simply read the retraction notice the journal put out rather than the authors' statement it is clear **the paper was rejected because the authors were found to have engaged extensively in one what their previous work has defined as questionable research practices**. Of course questionable research practices are *questionable* but when a group of experts on the subject engage in them repeatedly to push a claim it makes you start to question. It is very important to realize that, much like a business school professor selling books based on fraud... several key authors have every reason to generate and protect this finding.   

# Feeling Not so HOT (Honest, Open and Transparent)
The reason I'm writing this damn blog post after putting it off for a year is precisely because of this post and Brian Nosek, the document it contains, and the myths it is spurring about the reasons for retraction. 

![](/assets/img/nosek.png){: width="750" }

In this post and in the [OSF File](https://osf.io/4k5sf), Brian seems to suggest that the problems mostly surrounded a single line in the paper. He provides no direct link to the retraction notice, does not engage with the content of the retration notice and does not engage with the specific issues and framing in our MA which describes the reasons why the paper was retracted. Instead, he focuses on something we mention only in passing, the inaccuracy of this line in their paper: 

>"All confirmatory tests, replications and analyses were preregistered both in the individual studies (Supplementary Information section 3 and Supplementary Table 2) and for this meta-project (https://osf.io/6t9vm)."


Read this in contrast with the journal's exact language for why the retraction occurred. It is downright deceptive to describe the retraction in this manner. The issue he highlights as the cause for retraction is only a minor point that could have been fixed with a correction. No one felt this was cause enough to retract from the beginning. It would not have taken ten months and four reviewers to sort this out.  I understand that he disagrees with the journal's reason for retracting but that is no excuse for failing to engage substantively with the journal's rationale and create the impression the cause was quite different. 

The authors post-game write-up, along with describing their ability to submit a fresh manuscript to fresh peer review as an R&R has been very effective spin. Commentators on twitter appear to think this just isn't a big deal. Everyone is making a mountain out of a molehill and the authors are setting a good example by "retracting, revising and resubmitting" (This too is misleading). 

![](/assets/img/spin.jpeg){: width="750" }


What is striking is that **even now** the authors can't get their story straight on what they preregistered. Later in [the note](https://osf.io/2s94g/), they explain how this specific language came about and highlight that elsewhere they've described things accurately:

![](/assets/img/manyconfirmatory.png){: width="750" }

"Many" might leave you with the impression that this section accurately described the preregistered analyses. Nope. Let's take the very first analysis in that confirmatory analysis section.  It describes a multi-level meta-analysis that was coded up by James Pustejovsky: 

![](/assets/img/confirmatoryanalysis.png){: width="750" }

You can even check out the [pre-registered analysis code which describes this analysis as exploratory](https://osf.io/938rv/). As a fun aside, this preregistered code was developed by a [paid consultant](https://osf.io/rnvxk) they hired to provide feedback on the pre-registration and develop the pre-registration code. There's a whole [version history](https://osf.io/rnvxk) ostensibly created without access to data leading up to the presentation of results at Metascience 2019. Importantly, none of this code was released with original manuscript and only came to light after I raised the issue when sumbitting my concerns for investigation. So somehow in addition to forgetting what exactly they preregistered when writing it up, they forgot entirely that about this whole side-plot with a consultant they paid (who became a co-author) and just neglected to upload the code which would have made it much more obvious that the analyses were not preregisterd. What bad luck! 

Returning to the meta-analysis. 

You know how we really know this meta-analysis is exploratory though? Because [an old version](https://osf.io/rnvxk) of the paper clearly indicate they added this analysis in March of 2020 ("Deciphering the Decline Effect P6_JEP.docx‚Äù). Well after Schooler [gave a talk](https://www.youtube.com/watch?v=TUiUA5O0PFk) on a preliminary analysis of the data. Conducting an analysis after having analyzed the data and observed pattern is, by the authors own definitions, [not a confirmatory test](https://www.pnas.org/doi/10.1073/pnas.1708274114). It's a QRP to present it as one. It's worth highlighting that this meta-analysis is different from their analysis of variation in replicability across labs across labs, which their correction notice indicates was *also* exploratory and relabeled as confirmatory...

![](/assets/img/pustejovsky.png){: width="750" }

Hell, later in their published post-mortem they (maybe?) allude to this analysis not being preregistered by saying that "some sensitivity and heterogeneity analyses" were not preregistered. But that doesn't fix the earlier language which seems to imply this section described the preregistered analyses. Perhaps what they were thinking of was the section of  [their supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-023-01749-9/MediaObjects/41562_2023_1749_MOESM1_ESM.pdf) called "pre-registered analyses" which *does* describe the preregistration accurately. Unfortunately this is within a section also called cinformatory analyses which affirmatively claims that the replication rate and the exploratory-turned-confirmatory lab-specific variation were confirmatory. Why does this section differ from the main text? Who knows? Might be hard to keep your story straight about what you preregistered if you've abandoned your prereg long ago.  

All this is to say that if they mistakenly described what was preregistered, they did it in three distinct places and forgot to upload the code corresponding to the actually preregistered analysis which they describe accurately in the supplement. Hanlon's razor is having a hard time with this one. 


# It is not just about the preregistration 

Hopefully the above makes it dreadfully clear that either the authors are either unable or unwilling to be honest, open and transparent about what they preregistered, even now. As clearly demonstrated above, if the issue were simply that they whiffed the preregistration description in one spot or even a couple places---the journal would have said that, a little correction issued and this would have long ago blown over. The paper was retracted for many more reasons, a small subset of the easiest to explain are described below. 

## Outcome Switching and lying by omission. 
One of the key reasons this paper was retracted is that the authors do not accurately describe their original, pre-registered motivation for the study. It is one thing to forget to preregister but it is entirely different to simply switch outcomes and pretend you were studying that thing all along. 

Stephanie Lee's story covers the [supernatural hypothesis](https://www.chronicle.com/article/this-study-was-hailed-as-a-win-for-science-reform-now-its-being-retracted) that motivated the research and earned the funding from a parapsychology-friendly funder. Author Jonathan Schooler had long ago proposed that merely observing a phenomenon could change its effect size. Perhaps the other authors thought this was stupid, but that's a fantastic reason to either a) not be part of the project or b) write a separate preregistration for what *you* predict. We can see how the manuscript evolved to obscure this motivation for the study. The authors *were* somewhat transparent about their unconventional [supernatural explanation] in the early drafts of the paper as well. 

>‚ÄúAccording to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect)25. According to this account, the more studies run between the confirmation study and the self-replication, the greater the decline should be.‚Äù

This is nearly verbatim from the preregistration: 

>According to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect). Thus, we predict that the more studies run between the confirmation study and the self-replication, the greater will be the decline effect.

It is also found in [responses to reviewers](https://osf.io/rnvxk) at Nature, who sensed the authors were testing a supernatural idea even though they had reframed things towards replication by this point: 

>‚ÄúThe short answer to the purpose of many of these features was to design the study a priori to address exotic possibilities for the decline effect that are at the fringes of scientific discourse‚Ä¶.‚Äù

As an aside, it's wild to call your co-authors the fringes of scientific discourse..... but they were generous enough to keep this idea around in the in the name of their OSF repo and the [original title](https://osf.io/rnvxk): 

> The replicability of newly discovered psychological findings over repeated replications

It's also in their [original discussion](https://osf.io/rnvxk): 

>  Importantly, the number of times the initial investigators replicated an effect themselves is not predictive of the replicability of the effect by independent teams (Kunert, 2016). 

All of this erodes en route to the the published manuscript. Instead, the authors refer to these primary hypotheses that date back to the origin of the project as phenomena of secondary interest and do not describe the hypotheses and mechanisms explicitly. They refer only to this original motivation in the supplement of "test of unusual possible explanations."

![](/assets/img/secondaryinterest.png){: width="750" }


Why not? Well, no need to speculate. The authors say exactly why in their response to Tal Yarkoni who asked them in review at Nature to come out and say it if they were testing a supernatural hypothesis. They decided to just change the message of the paper. 

>‚ÄúAs such we revised the manuscript so as not to dwell on the decline effect hypotheses and kept that discussion and motivation for the design elements to the SOM.‚Äù

It's fine to realize your idea was bad, but something else to try bury it in the supplement and write up a whole different paper you describe in multiple places as being preregistered and what you set out to study. When you read the paper, you'd have no idea this is what they got funding to study. Their omitted variables and undisclosed deviations in their main-text statistical models make it even harder to discern they were after the decline effect. They were only found in the pre-registered analysis code which was made public during the investigation. 

In distancing themselves for [two of the three reasons they got funding](https://www.fetzer-franklin-fund.org/projects/deciphering-the-decline-effect-1/) they mislead the reader about what they set out to study and why. This isn't a preregistration issue. This is outcome switching, and lying by omission. The third reason, most closely aligned with their goals is the "creation of a gold standard" for conducting research. While this very much aligns with the message of the paper, it is really important to note that the evaluation criteria boils down to if they fail to observe the supernatural effect they'll have succeeded. If I don't find a unicorn today, I've solved cold fusion. See our matters arising if you want to sort out why this is bad logic. 

The authors seem to indicate in their post-game analysis that they had planned this framing and analysis all along. If so, it's damn surprising that none of this came up in the investigation. Why not include a link to literally any evidence of the claims in this paragraph? You think if you're on the hook for outcome switching and HARKing you could toss in a URL to provide evidence to the contrary. 

![](/assets/img/bestlaidplans.png){: width="750"}

Perhaps they're referring to their [operations manual](https://osf.io/ahdy7/files/osfstorage) but I certainly hope not. There is no plan to in that document that matches the description above. Here's there predictions, make of them what you will:

![](/assets/img/predictions.png){: width="750"}

If there is some time-stamped documentation that clearly states a desire to measure replicability---much less how they'll do so---I cannot fathom why they did not produce it during the investigation or when I reached out in November. Perhaps it exists, but other evidence suggests otherwise. What evidence? Well, I'm glad you asked. 

## HARKing, Hacking and Cherry-Picking
What really chaps my ass about the tweet indicating this is just a procedural error with preregistration is is does a little bit of QRP razzle-dazzle. The problem isn't failing to preregister *per se* but the reason these folks have espoused preregistraiton and built careers on it over a decade. If you alter your analysis in response to data to acheive a desired result, that result is less trustworthy. If you fail to disclose that, you're in a tight spot. If you actively pretend you did not do it... in multiple places....

So did they alter their analysis in response to data? Sure seems like it. 

Replicability was not found in the original preregistration. Nor is it found in the [analysis code dated 09/03/2019](https://osf.io/rnvxk). The first mention of estimating replicability was during Schooler‚Äôs talk on 09/05/2019, which estimated replicability as the proportion of significant results among the studies with significant confirmatory tests. 

![](/assets/img/schoolertalk.png){: width="750" }

Initially, they seemed to have been viewing the significant replications of null confirmatory findings as separate from their estimate of replicability. They were *surprised* by this result and Schooler frames it as an *incline effect*. 

Between Schooler's talk and the first draft of the write-up they wind up with [this abstract and general set of conclusions](https://osf.io/rnvxk) from their analysis: 

![](/assets/img/originalabstract.png){: width="750" }


This abstract for the earliest version of the manuscript states the replication rate is 92.2% (59/64) based on a definition that considers null replications of null confirmatory studies as ‚Äúsuccessful‚Äù. This is the opposite of the published definition which only considers significant replications of null confirmation studies as successful.  

![](/assets/img/originaldefinition.png){: width="750" }

There are a few things to note.  Their original conclusion is that "high-powered pre-registered studies can be effectively replicated by high-powered pre-registered replications". This makes sense because they are replicating the confirmatory studies in the original drafts framing---which were high powered. In the published version they've broadened this conclude that "This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries." Where did that framing come from? If this was the goal all along you'd think whoever wrote this draft would be aware.

Their definition of replicability used in the final version is not referred to here as replicability but simply statistically significant results. The reason they're not calling this replicability is because initially they're considering the outcome of the confirmatory study when estimating replicability. This accounts for the "highly powered pre-registered studies" language in their original abstract. 

The trouble is that this 92.2% appears to be a mathematical error, as it includes 47 of the 52 replications of significant confirmatory studies and appears to include all 12 replications of null confirmatory studies regardless of outcome. When corrected, this definition would produce a replicability rate would be 79% (51/64). This would be considerably lower than this drafts' chosen method of calculating statistical power (~100%), and lower than replicability in the published version (86%). It would also be lower than the highest estimates from the literature cited in the initial version and later omitted for reasons unstated (11/13 or 85% from Klein et al 2014). These are the two arguments they based the title on and they fall apart once you fix this little calculation error. 

But that shiny 84.4% rate of significance is sitting right there! So now, it's no longer the confirmatory studies they're replicating but the predicted direction from the pilots they refuse to make public. This gets around the "incline effect" and awkwardness of cases like the Redemption study's apparent direction reversal. The trouble is that this is not a metric anyone would plan *a priori* because it is far too sensitive to the base rate. Imagine they had only found 8 "true" findings. 100% significant results in replication for these true findings and 100% replication nulls for the null confirmatory studies. Their measure would indicate 50% replicability---the same as they say is typical in the literature. When you have perfect agreement between statistical decisions arising from replications and confirmations you wind up just measuring the base rate. If it's 10%? Replicability is 10%. This measure only indicates high replicability because the base rate is so high. 

Ok, but its not just this choice of measure that is a problem. As versions go on, they do a few things to bring their replicability above their power and the replicability in the literature. First, they revise their estimate of statistical power in a very strange way such that statistical power is zero when the sign of confirmatory test differs from predicted value even though the tests are two-tailed and the theoretical minimum is 5%. They lump in the near-1.0 statistical power estimates for significant confirmatory studies with this weird, at times zero, measure for null studies and goose statistical power down below their absurd and clearly opportunistic replicability estimate. Taken together, their claim that their rate of replication exceeds the theoretical maximum only arises as a result of jointly evolving their estimate of power and replicability. This is Hacking statistics to make a claim. 

Finally, they even alter what they define as the literature-wide rate of replicability they compare against. A key claim is that they observed high replicability and the literature has not. In the main text cite a range of replicability (30-70%) which they seem to average at 50%. You might think the approximately 50% here is just the halfway point between 30 and 70. Nope, the talk by [Schooler] lays out their original math.

![](/assets/img/fortyseven.png){: width="750" }

There are plenty of problems with this, not the least of which is that several of these studies involve non-random sampling frames and the estimate of replicability is a result of the chosen mix of well-replicated and novel findings. But the real rub is Many Labs 1. 10 out of 13 is 77%. In reality it's [11 out of 13](https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178) or 85% using their criteria of significance. Ooops, high replicability was achieved a decade ago replicating the open literature. Can't have that. 

From the initial analysis to subsequent versions, they cite ML2 and ML3 but all of a sudden leave out this study which would have undermined that claim they were higher than the disappointing range in the literature. Their 92.2% might have looked better, but the 86% is right there with it. They also opt to leave out [Soto 2019](https://journals.sagepub.com/doi/abs/10.1177/0956797619831612) which has 87% successful replicability. It's hard to make a claim that rigor-enhancing techniques are necessary for replicability when two other studies have replicated portions of the literature at similar rates which most assuredly did not embrace these techniques in discovery. Perhaps they didn't know Soto's work, but they certainly knew Many Labs 1. The two projects shared authors. If there was some justified reason for selectively excluding mid write-up the one study in the literature that makes it impossible for them to make the claim... that would be a stroke of luck. This is a very weird QRP in parallel to selective removing of outliers. Maybe they'd argue it's not a QRP because it isn't a formal statistical test, but that didn't stop them from anchoring the titular claim on this comparison. 

So they cherry-picked their point of comparison, selectively omitted two conflicting points of comparison from their original framing, and hacked their estimates of power and replicability to be near one another. We really don't have to speculate that they cherry-picked. They come out and say the quiet part loud in an [earlier draft](https://osf.io/rnvxk). Here, their 75% confidence interval is lower than their their other metrics and they suggest that because it *should* have been high it should no longer be used as a metric. 

![](/assets/img/lowest.png){: width="750" }


## Null hacking??

During the investigation, the authors uploaded an apparently undisclosed sensitivity analysis of some sort (Decline effects/Decline effects - temporal decline and blinding.Rmd). The file appears last modified on March 15th, 2020 shortly after the earliest provided draft of the paper (March 14th, 2020). This code involved four different specifications from the preregistered model, evaluated in various ways: without the blinding effect, without self-replications, independent replications only, and controlling for independent replications. 

Although these analyses are not described in the published version, they appear to explore the possible model space to (generously) probe whether the results are consistent across model specifications. In the variation that ignores self-replications, the authors do find an interaction effect between wave and blinding that is significant and consistently so across model specifications (see below). It is unclear why the model space was explored so thoroughly and yet these analyses (and their conflicting results) were not included in the write-up. It also seems like there's gotta be some name for running a model a bunch of different ways like a specification analysis and selectively presenting the results... 

![](/assets/img/nullhacking.png){: width="750" }



## Omission

Frankly, all of the above could probably be weaseled around. Is it really a formal analyses? These are just descriptives. Everyone engages in QRPs so what's the big deal here? Maybe the results are still good? Isn't this obviously true so who cares if the authors embraced all of those things the authors are arguing need to be eradicated for science to work... 

Here's the *absolutely essential* problem. They are contending that they discovered a set of highly replicable findings when using rigor-enhancing best practices. Because the discoveries occurred during the piloting phase, this can *only* be logically true if they followed the practices in piloting. Because they committed to doing every hypothesis selected from piloting, the replicability of this body of literature was decided in piloting. What's more, their estimate of replicability is with regard to the directions observed in the pilot phase--the only bit of data they seem utterly unwilling to make public. 

You can read more about the problem in depth as written [by Hullman](http://users.eecs.northwestern.edu/~jhullman/Hullman_Protzko_et_al_comments.pdf), who reviewed the Matters arising and the concerns. Tal Yarkoni, at Nature, pointed out the need to be transparent about the piloting process 4 years ago. As you read the paper, ask yourself the following questions: 

- Did they verify all pilots were pre-registered and conducted without deviations?
- How many hypotheses were tested per pilot? 
- Were they allowed to enter exploratory, post-hoc analyses based on trends in the pilot? 
- What were the actual sample sizes of the pilots? 
- Etc... 

It's odd they don't seem to note whether the *whole* pilot had to be nominated or single hypotheses from many outcomes could be selected. What few pilots I've found by weird google searches suggested they tested a lot of things and picked one of the hypotheses to pull forward. We get a hint of the flexibility from a post of Nosek's that rigor-enhancing best practices were not strictly required during the pilot. We also get an indication that effect sizes declined where we'd expect them to and the authors really didn't feel the need to mention this in the write-up. 

![](/assets/img/nosexplore.png){: width="750" }


Think about this for a second. If they found 16 hypotheses dredging out significant findings from some unstated number of n=200 experiments, committed to replicating them, and got 80% replicability or whatever... it implies that best practices are very much not needed to find replicable results. Or you can dredge significant findings and replicate them with studies at n=200. 

This is a *Huge* question on which the entire paper depends. Tal Pointed it out in his review. James, their hired consultant, brought up the pilot selection problem in comments on drafts of the [original preregistration in ~2018](https://osf.io/rnvxk). Why ignore your hired statistican, reviewers, and the concerns that might retract your paper and *still* refuse to click "make public" on those pilot repositories? Their post-mortem suggests they're analyzing the data, they must be organized somewhere, so why are they so reluctant to make them openly available? 

This was a key reason for retraction, as indicated in [Hullman's review](http://users.eecs.northwestern.edu/~jhullman/Hullman_Protzko_et_al_comments.pdf). The authors have been aware of the concerns for 4-6 years. It was emphasized in our matters arising in raised in the concerns that prompted the investigation. Against this backdrop, it seems pretty damn misleading for their post-mortem to say: 

> Also, our findings spurred interest in what occurred in the pilot phase of the project. We are likewise quite interested to learn more about the pilot phase.

It is very difficult to square this with openness, honesty and transparency. 

# Putting it all together
As I said at the outset. I had no desire to write this blog post. I didn't want to wade into all of this. Their OSF is such a mess that getting claims straight and conveying them to others is a nightmare. Many of the links above will just open a massive zip file you'll have to navigate to find the corresponding file. Good luck, go with god.  Other parts of their repo including pilots  of are unlinked and can only be found by searching on the internet archive, finding a registration and working your way back up---often to a still private repository. And despite the fact that embargos for many pilots should have lifted if they were actually preregistered, I can't find most by searching in this manner. At times, preregistration seem to be little more than word documented uploaded to an OSF that they could easily remove or edit. Sometimes it's just a description in a wiki.

I have little doubt some will remain unconvinced because this post was tl;dr and the files are td;dr (too disorganized, didn't read). I expect the authors and probably Lakens to try to seize on a thing or two I got wrong somewhere in here because the repo is a mess and they haven't adressed any of these concerns.  The inevitable bad faith response this will trigger is one of the very many reasons I did not want to write this post. I hate looking forward to some long screed of a blog post where a red square says "Oh but they measured replication in the only logical way to do so, I am not very impressed by Joe's scientific abilities" or whatnot. 

All this is to say if I'm wrong about anything I'm happy to correct or amend. But even if one or two things could be written or framed diffently, a lot would have to be wrong for it to meaningfully change the story. Still, of you doubt what's above, rest assured that this is an abbreviated version of the concerns I raised which were independently verified by a ten month investigation involving numerous experts. 

I summarized these concerns, and wrote the blog post I did not want to write, in order to convey that the reasonf or retraction was very much more than the preregistration issues. It was a pattern of misleading statements throughout the piece that obscured a research process so rife with QRPs the results could not be trusted. It was refusal to make public key data that were necessary to validate the claims. I assure you I have only scratched the surface of the issues in this too-long-but-its-time-for-work blog post.  Their utterly misleading description of why the paper was preregistered undermines the results of a 10 month investigation involving plenty of journal staff and four experts in the field--none of whom were myself or Berna. The reason we went that route--in lieu of detailing concerns from the beginning is because *I really did not want to write this post*. 